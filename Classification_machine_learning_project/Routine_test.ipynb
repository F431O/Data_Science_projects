{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOoXGcz53/xd87iBwoPGHW4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IGf0iCQcLPpB"},"outputs":[],"source":["#IMPORTAZIONE DATASET\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","%cd /content/drive/MyDrive/Project\\ ML"]},{"cell_type":"code","source":["#IMPORTAZIONE PACCHETTI\n","!pip install mlxtend\n","\n","import sklearn\n","import pandas as pd\n","import numpy as np\n","import joblib\n","import sys\n","\n","sys.modules['sklearn.externals.joblib'] = joblib\n","\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler, OneHotEncoder, OrdinalEncoder\n","from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n","from imblearn.over_sampling import RandomOverSampler, SMOTE\n","from sklearn.ensemble import IsolationForest\n","from sklearn.compose import ColumnTransformer\n","from sklearn.feature_selection import f_classif, mutual_info_classif, SelectKBest, SelectFromModel\n","from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.tree import DecisionTreeClassifier,export_graphviz\n","from sklearn import tree\n","import graphviz\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n","\n","np.random.seed = 123"],"metadata":{"id":"-9f5QNxMLblu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#LETTURA DATASET\n","data = pd.read_csv(\"test_set.csv\")\n","data.columns = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\n","                \"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\n","                \"capital-gain\",\"capital-loss\",\"hours-per-week\",\n","                \"native-country\",\"earns\"]\n","\n","print(data.head())\n","print(len(data))\n","print(np.unique(data[\"earns\"].to_numpy(), return_counts=True))\n","print(\"# of NaN values for each column:\")\n","print(data.isnull().sum(axis=0))"],"metadata":{"id":"STtQ9zhoLeQ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#PREPARAZIONE TRAINING SET E TEST SET\n","x = data.to_numpy()[:,:-1]\n","y = data.to_numpy()[:,-1]\n","print(x[:10,:])\n","x_tr, x_ts, y_tr, y_ts = train_test_split(x,y,test_size=0.4,stratify=y)\n","print(\"Shape of x_tr {}\".format(x_tr.shape))\n","print(\"Shape of y_tr {}\".format(y_tr.shape))\n","print(\"Shape of x_ts {}\".format(x_ts.shape))\n","print(\"Shape of y_ts {}\".format(y_ts.shape))\n","print(np.unique(y_tr, return_counts=True)[1]/len(y_tr))\n","print(np.unique(y_ts, return_counts=True)[1]/len(y_ts))\n","\n","#Trasformazione delle y di trainning e set in interi, quindi la variabile categorica \"<=50K\" diventa 0 mentre la variabile \">50K\" diventa 1\n","y_tr_1 = pd.DataFrame(y_tr).replace(\" <=50K\", 0)\n","y_tr_1 = pd.DataFrame(y_tr_1).replace(\" >50K\", 1)\n","y_tr_processed = y_tr_1.to_numpy()\n","print(np.unique(y_tr, return_counts=True))\n","print(np.unique(y_tr_processed, return_counts=True))\n","y_ts_1 = pd.DataFrame(y_ts).replace(\" <=50K\", 0)\n","y_ts_1 = pd.DataFrame(y_ts_1).replace(\" >50K\", 1)\n","y_ts_processed = y_ts_1.to_numpy()\n","print(np.unique(y_ts, return_counts=True))\n","print(np.unique(y_ts_processed, return_counts=True))"],"metadata":{"id":"bjyWEfopLq3C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Siccome il dataset è composto da variabili sia continue che categoriche, sono state costruite due pipeline che trattano le variabili in modo diverso. \n","#Per quanto riguarda le variabili continue è stata effettuata l'imputazone utilizzando il KNNImputer (nel dataset di trining non ci sono valori nan tra le variabili continue ma è stato inserito nel caso il dataset di test dovesse averle)\n","#successivamente viene eseguito il RandomScalar per normalizzare le variabili\n","\n","#per quanto riguarda le variabili categoriche per prima cosa viene eseguita l'imputazione dei valori mancanti (che sono segnati conil simbolo ?) utilizzando il SimpleImputer con strategia del most frequent\n","#e successivamente visne viene eseguito il OneHotEncoder in modo da trasformare le variabili categoriche inn variabili numeriche\n","\n","#pipeline per variabili numeriche\n","numerical = [\"age\",\"fnlwgt\",\"education-num\",\"capital-gain\",\"capital-loss\",\n","             \"hours-per-week\"] # nome delle variabili numeriche\n","numerical_transformer = Pipeline(steps=[\n","                                        ('imputer', KNNImputer(\n","                                            missing_values=np.nan,\n","                                            n_neighbors=3,\n","                                            weights=\"uniform\")),\n","                                        ('scaler', RobustScaler())])\n","\n","#pipeline per variabili categoriche\n","categorical_one_hot = [\"workclass\",\"education\",\"marital-status\",\"occupation\",\n","                       \"relationship\",\"race\",\"sex\",\n","                       \"native-country\"] # nome delle variabili categoriche\n","onehot_transformer = Pipeline(steps=[\n","                                    ('imputer', SimpleImputer(\n","                                        strategy='most_frequent', \n","                                        fill_value='missing', \n","                                        missing_values=\" ?\")),\n","                                    ('encoder', OneHotEncoder(\n","                                        handle_unknown='ignore'))])\n","\n","#esecuzione delle divesre pipeline a seconda della variabile\n","preprocessing = ColumnTransformer(\n","    transformers=[\n","                  ('num', numerical_transformer, numerical),\n","                  ('onehot', onehot_transformer, categorical_one_hot)\n","                  ])\n","\n","x_tr_pd = pd.DataFrame(x_tr)\n","x_ts_pd = pd.DataFrame(x_ts)\n","x_tr_pd.columns = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\n","                   \"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\n","                   \"capital-gain\",\"capital-loss\",\"hours-per-week\",\n","                   \"native-country\"]\n","x_ts_pd.columns = [\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\n","                   \"marital-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\n","                   \"capital-gain\",\"capital-loss\",\"hours-per-week\",\n","                   \"native-country\"]\n","\n","x_tr_processed = preprocessing.fit_transform(x_tr_pd).toarray() #addestramento (e utilizzo) delle operazioni di pre-processing sul trainig set\n","x_ts_processed = preprocessing.transform(x_ts_pd).toarray() #utilizzo delle operazioni di pre-processing sul test set\n","print(x_tr_processed)\n","print(x_ts_processed)\n","print(\"Media distribuzione di partenza: {}\".format(np.mean(x_tr_pd, axis=0)))\n","print(\"Media distribuzione scalata: {}\".format(np.mean(x_tr_processed, axis=0)))\n","print(\"Media distribuzione di partenza: {}\".format(np.mean(x_ts_pd, axis=0)))\n","print(\"Media distribuzione scalata: {}\".format(np.mean(x_ts_processed, axis=0)))"],"metadata":{"id":"ygt2lhPfLuUU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#BALANCING \n","\n","#Visualizzazone dello sbilanciamento del dataset\n","n_classes = len(np.unique(y_tr_processed))\n","fig, ax = plt.subplots()\n","sizes = [len(y_tr_processed[y_tr_processed==i]) for i in range(n_classes)]\n","print(sizes)\n","class_names = [chr(ord('A')+i) for i in range(n_classes)]\n","ax.pie(sizes, labels=class_names, autopct='%1.1f%%',shadow=True,startangle=90)\n","ax.axis(\"equal\")\n","plt.show()\n","\n","\n","#ANOMALY DETECTION CON ISOLATION FOREST (metodo di bilanciamento usato = SMOTE)\n","print(\"Initial distribution\")\n","print(len(x_tr_pd))\n","print(np.unique(y_tr_processed, return_counts=True))\n","print(np.unique(y_tr_processed, return_counts=True)[1]/len(y_tr_processed))\n","\n","anomaly_detector = IsolationForest()\n","anomaly_detector.fit(x_tr_processed)\n","is_inlier = anomaly_detector.predict(x_tr_processed)\n","\n","print(np.unique(is_inlier, return_counts=True))\n","x_tr_not_anomalous = x_tr_processed[is_inlier==1,:]\n","y_tr_not_anomalous = y_tr_processed[is_inlier==1]\n","\n","print(\"Distribution after anomaly detection\")\n","print(x_tr_not_anomalous.shape[0])\n","print(np.unique(y_tr_not_anomalous, return_counts=True)[1]/len(\n","    y_tr_not_anomalous))\n","\n","balancer = SMOTE(random_state=42)\n","x_tr_balanced, y_tr_balanced = balancer.fit_resample(\n","    x_tr_not_anomalous, y_tr_not_anomalous)\n","\n","print(\"Distribution after balancing\")\n","print(x_tr_balanced.shape[0])\n","print(np.unique(y_tr_balanced, return_counts=True)[1]/len(y_tr_balanced))\n","\n","\n","#Visualizzazione dataset bilanciato\n","fig, ax = plt.subplots()\n","sizes = [len(y_tr_balanced[y_tr_balanced==i]) for i in range(n_classes)]\n","class_names = [chr(ord(\"A\")+i) for i in range(n_classes)]\n","ax.pie(sizes, labels = class_names, autopct = \"%1.1f%%\", shadow = True, \n","       startangle = 90)\n","ax.axis(\"equal\")\n","plt.show()"],"metadata":{"id":"TFDzDIkjLy6h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#funzione che addestra il modello di classificazione scelto con il trainig set e stampa le varie misure di accuratezza oltre alla confusion matrix\n","def train_evaluate(model, x_tr, y_tr, x_ts, y_ts):\n","  model.fit(x_tr, y_tr)\n","  y_tr_pred = model.predict(x_tr)\n","  y_ts_pred = model.predict(x_ts)\n","\n","  print(\"Confusion matrix on training set\")\n","  print(confusion_matrix(y_tr, y_tr_pred))\n","  print(\"Confusion matrix on test set\")\n","  print(confusion_matrix(y_ts, y_ts_pred))\n","  print(\"Accuracy on training set: {}\".format(accuracy_score(y_tr, y_tr_pred)))\n","  print(\"Accuracy on test set: {}\".format(accuracy_score(y_ts, y_ts_pred)))\n","  print(\"F1 score on training set: {}\".format(f1_score(y_tr, y_tr_pred)))\n","  print(\"F1 score on test set: {}\".format(f1_score(y_ts, y_ts_pred)))"],"metadata":{"id":"6iIAY5MUL2I6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#CLASSIFICAZIONE\n","print(\"Gradient Boosting\")\n","classifier = GradientBoostingClassifier(n_estimators=100, max_depth=5)\n","print(\"---RESULT---\")\n","train_evaluate(\n","    classifier, x_tr_balanced, y_tr_balanced, x_ts_processed, y_ts_processed)"],"metadata":{"id":"wXwJnD1OL44d"},"execution_count":null,"outputs":[]}]}